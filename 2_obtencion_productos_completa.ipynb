{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56bf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests BeautifulSoup csv json time random\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc646538",
   "metadata": {},
   "source": [
    "# 1. Paginación y scraping de múltiples páginas\n",
    "En este caso vamos a tomar la url que nos general al interactuar con las diferentes paginas de la web, para así obtener todos sus productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2ce445",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/catalogue/category/books_1/page-{}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b04b01",
   "metadata": {},
   "source": [
    "Recorremos todas la paginas para guardar los productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84c4b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 1 procesada.\n",
      "Pagina 2 procesada.\n",
      "Pagina 3 procesada.\n",
      "Pagina 4 procesada.\n",
      "Pagina 5 procesada.\n",
      "Pagina 6 procesada.\n",
      "Pagina 7 procesada.\n",
      "Pagina 8 procesada.\n",
      "Pagina 9 procesada.\n",
      "Pagina 10 procesada.\n",
      "Pagina 11 procesada.\n",
      "Pagina 12 procesada.\n",
      "Pagina 13 procesada.\n",
      "Pagina 14 procesada.\n",
      "Pagina 15 procesada.\n",
      "Pagina 16 procesada.\n",
      "Pagina 17 procesada.\n",
      "Pagina 18 procesada.\n",
      "Pagina 19 procesada.\n",
      "Pagina 20 procesada.\n",
      "Pagina 21 procesada.\n",
      "Pagina 22 procesada.\n",
      "Pagina 23 procesada.\n",
      "Pagina 24 procesada.\n",
      "Pagina 25 procesada.\n",
      "Pagina 26 procesada.\n",
      "Pagina 27 procesada.\n",
      "Pagina 28 procesada.\n",
      "Pagina 29 procesada.\n",
      "Pagina 30 procesada.\n",
      "Pagina 31 procesada.\n",
      "Pagina 32 procesada.\n",
      "Pagina 33 procesada.\n",
      "Pagina 34 procesada.\n",
      "Pagina 35 procesada.\n",
      "Pagina 36 procesada.\n",
      "Pagina 37 procesada.\n",
      "Pagina 38 procesada.\n",
      "Pagina 39 procesada.\n",
      "Pagina 40 procesada.\n",
      "Pagina 41 procesada.\n",
      "Pagina 42 procesada.\n",
      "Pagina 43 procesada.\n",
      "Pagina 44 procesada.\n",
      "Pagina 45 procesada.\n",
      "Pagina 46 procesada.\n",
      "Pagina 47 procesada.\n",
      "Pagina 48 procesada.\n",
      "Pagina 49 procesada.\n"
     ]
    }
   ],
   "source": [
    "product_list = [] #  Lista de productos\n",
    "\n",
    "# recorremos las 4 pestañas\n",
    "for page in range(1, 50):\n",
    "    url = base_url.format(page) #  formateamos la base url para añadir las paginas\n",
    "    response = requests.get(url)  # obtenemos la información de la pagina\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # parseamos el html obtenido\n",
    "    products = soup.select(\"article.product_pod\") #  Seleccionamos la lista de productos en su elemento\n",
    "\n",
    "\n",
    "\n",
    "    # procesamos cada producto\n",
    "    for product in products:\n",
    "        title = product.find(\"h3\").find(\"a\")[\"title\"]\n",
    "        price = product.find(\"p\", class_=\"price_color\").get_text()\n",
    "        image_rel = product.find(\"div\", class_=\"image_container\").find(\"img\")[\"src\"]\n",
    "        img_url = \"http://books.toscrape.com/\" + image_rel\n",
    "\n",
    "        # prearar el json\n",
    "        product_list.append({\n",
    "            \"title\": title,\n",
    "            \"price\": price,\n",
    "            \"img_url\": img_url\n",
    "        })\n",
    "\n",
    "    # Se deja el sleep al mismo nivel del primer ciclo\n",
    "\n",
    "    # Espera breve entre paginas, así se simula navegación Real\n",
    "    time.sleep(1)\n",
    "    print(f\"Pagina {page} procesada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cc6e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping multipágina completado: 980 productos guardados en el documento.\n"
     ]
    }
   ],
   "source": [
    "with open(\"resultados/productos_mult_page.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"price\", \"img_url\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"Scraping multipágina completado: {len(product_list)} productos guardados en el documento.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863790c",
   "metadata": {},
   "source": [
    "# 2. Manejo de excepciones y cambios de url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3178dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina 47 procesada.\n",
      "Pagina 48 procesada.\n",
      "Pagina 49 procesada.\n",
      "Pagina 50 procesada.\n",
      "Error en la página 51: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-51.html\n",
      "Error en la página 52: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-52.html\n",
      "Error en la página 53: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-53.html\n",
      "Error en la página 54: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-54.html\n",
      "Error en la página 55: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-55.html\n",
      "Error en la página 56: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-56.html\n",
      "Error en la página 57: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-57.html\n",
      "Error en la página 58: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-58.html\n",
      "Error en la página 59: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-59.html\n",
      "Error en la página 60: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-60.html\n",
      "Error en la página 61: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-61.html\n",
      "Error en la página 62: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-62.html\n",
      "Error en la página 63: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-63.html\n",
      "Error en la página 64: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-64.html\n",
      "Error en la página 65: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-65.html\n",
      "Error en la página 66: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-66.html\n",
      "Error en la página 67: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-67.html\n",
      "Error en la página 68: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-68.html\n"
     ]
    }
   ],
   "source": [
    "product_list = []  # Lista de productos\n",
    "\n",
    "# recorremos las 4 pestañas\n",
    "for page in range(47, 69):\n",
    "    # formateamos la base url para añadir las paginas\n",
    "    url = base_url.format(page)\n",
    "\n",
    "    # añadiremos persistencia ante errores\n",
    "    try:    \n",
    "        response = requests.get(url)  # obtenemos la información de la pagina\n",
    "        # IMPORTANTE\n",
    "        response.raise_for_status() #  Este lanza los codigos de status\n",
    "        # parseamos el html obtenido\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Seleccionamos la lista de productos en su elemento\n",
    "        products = soup.select(\"article.product_pod\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        # muestra el error\n",
    "        print(f\"Error en la página {page}: {e}\")\n",
    "        continue # continua a la siguiente iteración\n",
    "\n",
    "    # procesamos cada producto\n",
    "    for product in products:\n",
    "        try:\n",
    "            title = product.find(\"h3\").find(\"a\")[\"title\"]\n",
    "            price = product.find(\"p\", class_=\"price_color\").get_text()\n",
    "            image_rel = product.find(\n",
    "                \"div\", class_=\"image_container\").find(\"img\")[\"src\"]\n",
    "            img_url = \"http://books.toscrape.com/\" + image_rel\n",
    "\n",
    "            # prearar el json\n",
    "            product_list.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"img_url\": img_url\n",
    "            })\n",
    "\n",
    "        # Manejo de posibles errores y excepciones   \n",
    "        except Exception as ex:\n",
    "            print(\"Error extrayendo datos de un producto: \", ex)\n",
    "\n",
    "\n",
    "    # Se deja el sleep al mismo nivel del primer ciclo\n",
    "\n",
    "    # Espera breve entre paginas, así se simula navegación Real\n",
    "    time.sleep(1)\n",
    "    print(f\"Pagina {page} procesada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4051723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completa: 80 productos guardados\n"
     ]
    }
   ],
   "source": [
    "# guardar resultados en CSV\n",
    "path_csv = \"resultados/products_con errores.csv\"\n",
    "# newline -> agregar algo al principio\n",
    "with open(path_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    try:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"img_url\"])\n",
    "        writer.writeheader()  # crear cabercera con los fieldnames\n",
    "        writer.writerows(product_list)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la creación de archivo csv: {e}\")\n",
    "print(f\"Extracción completa: {len(product_list)} productos guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1130e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
